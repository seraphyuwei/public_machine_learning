{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a PySpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"name\").config('config','value').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The most common method used to load data is textFile. This method takes an URI for the file (local file or other URI like hdfs://), and will read the data in as a collections of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "raw_content = sc.textFile('2015-12-12.csv')\n",
    "\n",
    "# print the type of the object\n",
    "type(raw_content)\n",
    "raw_content.dtypes\n",
    "\n",
    "# print the number of lines\n",
    "raw_content.count()\n",
    "421970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the first n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the first 5 rows\n",
    "raw_content.take(5)\n",
    "\n",
    "[u'\"date\",\"time\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"',\n",
    " u'\"2015-12-12\",\"13:42:10\",257886,\"3.2.2\",\"i386\",\"mingw32\",\"HistData\",\"0.7-6\",\"CZ\",1',\n",
    " u'\"2015-12-12\",\"13:24:37\",1236751,\"3.2.2\",\"x86_64\",\"mingw32\",\"RJSONIO\",\"1.3-0\",\"DE\",2',\n",
    " u'\"2015-12-12\",\"13:42:35\",2077876,\"3.2.2\",\"i386\",\"mingw32\",\"UsingR\",\"2.0-5\",\"CZ\",1',\n",
    " u'\"2015-12-12\",\"13:42:01\",266724,\"3.2.2\",\"i386\",\"mingw32\",\"gridExtra\",\"2.0.0\",\"CZ\",1']\n",
    "\n",
    "# show the first row\n",
    "raw_content.first()\n",
    "\n",
    "[u'\"date\",\"time\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"']\n",
    "\n",
    "# take random samples\n",
    "raw_content.takeSample(if_replacement = True, number_of_samples = 5, seed = 3)\n",
    "\n",
    "[u'\"2015-12-12\",\"16:41:22\",18773,\"3.2.3\",\"x86_64\",\"mingw32\",\"evaluate\",\"0.8\",\"US\",10935',\n",
    " u'\"2015-12-12\",\"13:06:32\",494138,\"3.2.3\",\"x86_64\",\"linux-gnu\",\"rjson\",\"0.2.15\",\"KR\",655',\n",
    " u'\"2015-12-12\",\"03:50:05\",140207,NA,NA,NA,\"SACOBRA\",\"0.7\",\"DE\",129',\n",
    " u'\"2015-12-12\",\"21:40:13\",622505,\"3.2.3\",\"x86_64\",\"linux-gnu\",\"stratification\",\"2.2-5\",\"US\",4860',\n",
    " u'\"2015-12-12\",\"23:52:06\",805204,\"3.2.2\",\"x86_64\",\"mingw32\",\"readxl\",\"0.1.0\",\"CA\",104']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = raw_content.map(lambda x:x.split(','))\n",
    "content.take(3)\n",
    "\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "\n",
    "def clean(x):\n",
    "    return([xx.replace('\"','') for xx in x])\n",
    "content = content.map(clean)\n",
    "content.take(4)\n",
    "\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# difference between map and flatmap\n",
    "text = [\"a b c\", \"d e\", \"f g h\"]\n",
    "sc.parallelize(text).map(lambda x:x.split(\" \")).collect()\n",
    "[['a', 'b', 'c'], ['d', 'e'], ['f', 'g', 'h']]\n",
    "\n",
    "sc.parallelize(text).flatMap(lambda x:x.split(\" \")).collect()\n",
    "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce and counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 1: a -> (a,1), b -> (b,1)\n",
    "# step 2: (a, [1,1,1]), (b,[1,1,1,1,1,1])\n",
    "# step 3: (a,3), (b,5)\n",
    "package_count = content.map(lambda x:(x[6],1)).reduceByKey(lambda a,b:a+b)\n",
    "type(package_count)\n",
    "<class 'pyspark.rdd.PipelinedRDD'>\n",
    "\n",
    "package_count.count()\n",
    "8660\n",
    "\n",
    "package_count.take(5)\n",
    "[(u'SIS', 24), \n",
    "(u'StatMethRank', 15), \n",
    "(u'dbmss', 54), \n",
    "(u'searchable', 14), \n",
    "(u'RcmdrPlugin.TextMining', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can also use countByKey method. The result returned by it is in hashmap structure\n",
    "package_count_2 = content.map(lambda x:(x[6],1)).countByKey()\n",
    "type(package_count_2)\n",
    "<type 'collections.defaultdict'>\n",
    "\n",
    "package_count_2['ggplot2']\n",
    "3913\n",
    "\n",
    "package_count_2['stm']\n",
    "25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please note that countByKey method ONLY works on RDDs of type (K,V), returning a hashmap of (K,int) pairs\n",
    "# with the COUNT of each key [1]. And the value of V will NOT affect the result.\n",
    "package_count_2 = content.map(lambda x:(x[6],1)).countByKey()\n",
    "package_count_2['ggplot']\n",
    "3913\n",
    "\n",
    "package_count_2 = content.map(lambda x:(x[6],3)).countByKey()\n",
    "package_count_2['ggplot']\n",
    "3913\n",
    "\n",
    "package_count_2 = content.map(lambda x:(x[6],'test')).countByKey()\n",
    "package_count_2['ggplot']\n",
    "3913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After counting by reduce method, I may want to know the rankings of these packages based on how many downloads\n",
    "# they have. Then we need to use sortByKey method. Note:\n",
    "# the 'Key\" here refers to the first element of each array\n",
    "# the argument sortByKey(0 = descent, 1 = ascent)\n",
    "# sort DESCENTLY and get the first 10 rows\n",
    "package_count.map(lambda x:(x[1],x[0])).sortByKey(0).take(10)\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "\n",
    "# sort ascently and get the first 10 rows\n",
    "package_count.map(lambda x:(x[1],x[0])).sortByKey(1).take(10)\n",
    " [(1, u'TSjson'),\n",
    " (1, u'ebayesthresh'),\n",
    " (1, u'parspatstat'),\n",
    " (1, u'gppois'),\n",
    " (1, u'JMLSD'),\n",
    " (1, u'kBestShortestPaths'),\n",
    " (1, u'StVAR'),\n",
    " (1, u'mosaicManip'),\n",
    " (1, u'em2'),\n",
    " (1, u'DART')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other than sorting by key (normally it's the first element in each observation), \n",
    "# we can also specify by which element to sort using method sortBy\n",
    "package_connt.sortBy(lambda x:x[1]).take(5) #default ascending is True\n",
    "[(u'TSjson', 1),\n",
    " (u'ebayesthresh', 1),\n",
    " (u'parspatstat', 1),\n",
    " (u'gppois', 1),\n",
    " (u'JMLSD', 1)]\n",
    "\n",
    "package_count.sortBy(lambda x:x[1], ascending=False).take(5)\n",
    "[(u'Rcpp', 4783),\n",
    " (u'ggplot2', 3913),\n",
    " (u'stringi', 3748),\n",
    " (u'stringr', 3449),\n",
    " (u'plyr', 3436)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider filter as the SELECT * from TABLE WHERE ??? statement in SQL. It can help return a new dataset formed by selecting those elements of the source on which the function specified by user returns true.\n",
    "\n",
    "For example, I would want to obtain these downloading records of R package \"Rtts\" from China (CN), then the condition is \"package == 'Rtts' AND country = 'CN'\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content.filter(lambda x:x[6] == 'Rtts' and x[8] == 'CN').count()\n",
    "1\n",
    "\n",
    "content.filter(lambda x:x[6] == 'Rtts' and [8] == 'CN').take(1)\n",
    "[[u'2015-12-12', u'20:15:24', u'23820', u'3.2.2', u'x86_64', u'mingw32', u'Rtts', u'0.3.3', u'CN', u'41']]\n",
    "\n",
    "# already became a list\n",
    "temp = content.filter(lambda x:x[6] == 'Rtts' and [8] == 'CN').take(1)\n",
    "type(temp)\n",
    "<type 'list'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the set operators in Oracle SQL, we can do set operations in Spark. Here we would introduce union, intersection, and distinct. We can make intuitive interpretations as below.\n",
    "\n",
    "union of A and B: return elements of A AND elements of B.\n",
    "intersection of A and B: return these elements existing in both A and B.\n",
    "distinct of A: return the distinct values in A. That is, if element a appears more than once, it will only appear once in the result returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One point we need to take note of is that if each line of our data is an array instead of a string, \n",
    "# intersection and distinct methods can't work properly. \n",
    "# This is why I used raw_content instead of content here as example.\n",
    "raw_content.count()\n",
    "421970\n",
    "\n",
    "raw_content.union(raw_content).count()\n",
    "843940\n",
    "\n",
    "raw_content.intersection(raw_content).count()\n",
    "421553\n",
    "\n",
    "raw_content.distintct().count()\n",
    "421553"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, I have found the data process methods in Spark is quite similar to that in SQL, like I can use join method in Spark, which is a great news! Outer joins are also supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin [1]. Additionally, cartesian is available as well (please note Spark SQL is available for similar purpose and would be preferred & recommended).\n",
    "\n",
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key[1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a new RDD in which the 'country' variable is KEY\n",
    "content_modified=content.map(lambda x:(x[8], x))\n",
    "\n",
    "# give a mapping table of the abbreviates of four countries and their full name.\n",
    "mapping=[('DE', 'Germany'), ('US', 'United States'), ('CN', 'China'), ('IN',\"India\")]\n",
    "mapping=sc.parallelize(mapping)\n",
    "\n",
    "# join\n",
    "content_modified.join(mapping).takeSample(False, 8)\n",
    "[\n",
    "(u'CN', ([u'2015-12-12', u'19:26:01', u'512', u'NA', u'NA', u'NA', u'reweight', u'1.01', u'CN', u'4721'], 'China')), \n",
    "(u'US', ([u'2015-12-12', u'18:15:11', u'14271399', u'3.2.1', u'x86_64', u'mingw32', u'stringi', u'1.0-1', u'US', u'11837'], 'United States')), \n",
    "(u'US', ([u'2015-12-12', u'00:03:27', u'392370', u'3.2.3', u'x86_64', u'linux-gnu', u'colorspace', u'1.2-6', u'US', u'12607'], 'United States')), \n",
    "(u'US', ([u'2015-12-12', u'05:10:29', u'290932', u'3.2.2', u'x86_64', u'mingw32', u'iterators', u'1.0.8', u'US', u'5656'], 'United States')), \n",
    "(u'US', ([u'2015-12-12', u'22:28:47', u'2143454', u'3.2.3', u'x86_64', u'linux-gnu', u'quantreg', u'5.19', u'US', u'16318'], 'United States')), \n",
    "(u'US', ([u'2015-12-12', u'13:12:26', u'985806', u'3.2.3', u'x86_64', u'linux-gnu', u'plotly', u'2.0.3', u'US', u'2570'], 'United States')), \n",
    "(u'CN', ([u'2015-12-12', u'17:04:44', u'178399', u'3.2.1', u'x86_64', u'darwin13.4.0', u'apsrtable', u'0.8-8', u'CN', u'41'], 'China')), \n",
    "(u'US', ([u'2015-12-12', u'06:41:09', u'76007', u'3.2.3', u'i386', u'mingw32', u'superpc', u'1.09', u'US', u'1985'], 'United States'))\n",
    "]\n",
    "\n",
    "# left outer join. \n",
    "# In the mapping table, we only gave the mappings of four countries, so we found some 'None' values in the returned result below\n",
    "content_modified.leftOuterJoin(mapping).takeSample(False, 8)\n",
    "[\n",
    "(u'US', ([u'2015-12-12', u'15:43:03', u'153892', u'3.2.2', u'i386', u'mingw32', u'gridBase', u'0.4-7', u'US', u'8922'], 'United States')), \n",
    "(u'CN', ([u'2015-12-12', u'19:59:37', u'82833', u'3.2.3', u'x86_64', u'mingw32', u'rgcvpack', u'0.1-4', u'CN', u'41'], 'China')), \n",
    "(u'JP', ([u'2015-12-12', u'17:24:59', u'2677787', u'3.2.3', u'i386', u'mingw32', u'ggplot2', u'1.0.1', u'JP', u'3597'], None)), \n",
    "(u'TN', ([u'2015-12-12', u'13:40:13', u'1229084', u'3.2.2', u'x86_64', u'mingw32', u'forecast', u'6.2', u'TN', u'10847'], None)), \n",
    "(u'US', ([u'2015-12-12', u'05:09:59', u'75327', u'3.2.3', u'x86_64', u'mingw32', u'xml2', u'0.1.2', u'US', u'5530'], 'United States')), \n",
    "(u'AE', ([u'2015-12-12', u'14:23:56', u'695625', u'3.1.2', u'i386', u'mingw32', u'mbbefd', u'0.7', u'AE', u'556'], None)), \n",
    "(u'KR', ([u'2015-12-12', u'16:31:34', u'36701', u'3.2.3', u'x86_64', u'linux-gnu', u'ttScreening', u'1.5', u'KR', u'4986'], None)), \n",
    "(u'US', ([u'2015-12-12', u'15:43:08', u'35212', u'3.2.2', u'x86_64', u'mingw32', u'reshape2', u'1.4.1', u'US', u'8922'], 'United States'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select\n",
    "# show all entries in firstName column\n",
    "df.select(\"firstName\").show()\n",
    "df.select(\"firstName\",'lastName').show()\n",
    "\n",
    "# show all entries in firstName, age and type\n",
    "df.select(\"firstName\",'age',explode(\"phoneNumber\").alias(\"contactInfo\")).select(\"contactInfo.type\",\"firstName\",'age').show()\n",
    "\n",
    "# show all entries in firstName and age, add 1 to the entries of age\n",
    "df.select(df['firstName'],df['age']+1).show()\n",
    "\n",
    "# show all entries where age > 24\n",
    "df.select(df['age']>24).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when\n",
    "# show firstName and 0 or 1 depending on age > 30\n",
    "df.select(\"firstName\",F.when(df.age > 30,1).otherwise(0)).show()\n",
    "\n",
    "#show firstName if in the given options\n",
    "df[df.firstName.isin('Jane','Boris')].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# like\n",
    "# show firstName, and lastName is True if lastName is like Smith\n",
    "df.select(\"firstName\",df.lastName.like('Smith')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
